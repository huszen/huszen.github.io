{
  "gemini-chatbot": {
    "title": "Gemini PDF RAG",
    "image": "assets/images/portfolio/gemini_pdf_rag.png",
    "description": "Chat with any PDF using Gemini + LangChain. Upload your PDF, ask questions, and get contextual answers.",
    "link": "https://github.com/huszen/gemini_pdf_rag",
    "techs": ["LangChain", "Google AI Studio", "Streamlit", "Python"],
    "features": ["PDF upload and parsing", "RAG-based Q&A", "Streaming response", "Multi-file support"],
    "process": "I built this project because I wanted to explore whether I could use Google AI Studio for my own personal projects. Specifically, I was curious to know if it's possible to integrate the API from Google AI Studio into a custom application. Around the same time, I was also learning about LangChain, and the idea came to mind: maybe I could build something useful, lightweight, and free for myself. The main goal was to interact with PDF files using a large language model (LLM) without having to download or host any of the heavy models myself. Thanks to Google AI Studio, I could use their hosted model through an API key, which made everything much simpler.That’s how my journey to build this project started. The first major step was creating the core of a Retrieval-Augmented Generation (RAG) system using LangChain. I began by figuring out how to read and extract content from PDF files. Fortunately, LangChain supports the PyPDF library, which made it easy to load and process PDFs. After reading the file, the next step was embedding it—this means converting the content into a numerical format that the LLM can understand and retrieve from. Once embedded, I split the content into smaller chunks. These chunks act as the pieces of information that LangChain can retrieve later, and they serve as context for the model when answering user queries.After chunking the PDF, I created a custom prompt template. This prompt helps guide the LLM to respond more accurately by including not only the user's question but also the relevant context from the PDF. The result is a more focused and document-aware response. Although I attempted to add conversational memory to the system (so it could remember past interactions), I haven’t fully implemented that part yet.Once the core RAG logic was complete and working well, I moved on to building a simple web interface using Streamlit. This made the project easy to use—users can upload their PDF files, type their questions, and receive responses generated by Google’s language model, all within a lightweight app.One of the best things about this project is how lightweight it is. There's no need to download or host any large models—just an API key from Google AI Studio is enough. On the downside, I noticed that embedding very large PDF files might take quite a long time and may not yield the best results (though I haven’t fully tested that yet). Overall, I’m really happy with how it turned out: I built a working RAG system, learned more about LangChain, and proved that it’s possible to create useful AI tools with minimal resources.",
    "challenges": "One of the main challenges I faced was designing a user-friendly chat interface that provides a smooth and intuitive experience. I wanted the interaction to feel natural, especially since users would be engaging with an AI to ask questions about their documents. Another significant challenge was implementing a memory system. Ideally, the LLM should be able to remember previous messages so it can understand follow-up questions or references to earlier parts of the conversation. Building this kind of contextual memory is still a work in progress, as it requires maintaining and managing conversation history in a way that’s both efficient and compatible with the model’s input limitations.",
    "learnings": "Improved my understanding of LLMs, API token handling, and file parsing."
  },
  "keyphrase-generation": {
    "title": "Keyphrase Generation with T5 and BART",
    "image": "assets/images/portfolio/keyphrase_gen_assets.gif",
    "description": "Fine-tuned T5 and BART to automatically generate keyphrases for scientific documents.",
    "link": "https://github.com/huszen/keyphrase-generation-with-t5-and-bart-model",
    "techs": ["T5", "BART", "HuggingFace Transformers", "PyTorch"],
    "features": ["Supports both extraction & generation of keyphrases", "Trained on custom datasets", "F1 evaluation"],
    "process": "This project was part of my exploration into fine-tuning language models for keyphrase generation. My goal was to make the model generate keyphrases from scientific texts based on how I wanted it to behave. I used T5 and BART—both encoder-decoder transformers capable of understanding context and generating text. I fine-tuned these models using Hugging Face’s codebase, mostly adjusting and modifying existing scripts to fit my use case, so there wasn't much custom coding involved. For training, I used the KP20K dataset, which is commonly used for keyphrase generation tasks. Although the final results weren’t ideal—the F1 scores were lower than expected—it was a valuable learning experience that taught me the fundamentals of fine-tuning transformer models.",
    "challenges": "Finding optimal hyperparameters was difficult, and the long training time made it hard to experiment with different configurations.",
    "learnings": "Deepened my understanding of pre-trained language models and the fine-tuning process for custom NLP tasks."
  },
  "visit-campus": {
    "title": "Visit Campus",
    "image": "assets/images/portfolio/visitCampus_cover.png",
    "description": "A simple app to help you learn more about your dream college",
    "link": "https://github.com/sugama1001/VisitCampus",
    "techs": ["Python", "Tensorflow", "Flask"],
    "features": ["ChatBot", "Personality Test", "Information System"],
    "process": "This is our Bangkit Capstone Project. I was responsible for creating two machine learning models along with two of my peers. The first was a personality test model, designed to help users better understand themselves and discover which degree program might suit them. The second was a chatbot model, created to assist users in learning more about our application. After developing these models, our machine learning cohort also worked on building API endpoints to deliver the model outputs to end users. We used Flask to create the REST API for this purpose.",
    "challenges": "Building a robust personality prediction model; creating a chatbot model capable of answering specific questions about our application; developing API endpoints for both models, which was our first experience in API development.",
    "learnings": "Deepened my understanding of end-to-end production, from backend to frontend."
  },
  "british-airways-dashboard": {
    "title": "British Airways Dashboard with Tableau",
    "image": "assets/images/portfolio/british_airways_dashboard.png",
    "description": "An interactive Tableau dashboard visualizing British Airways data, with filtering and drill-down features.",
    "link": "https://public.tableau.com/app/profile/hasbi.hussein.deri/viz/my_airplane_analysis/Dashboard",
    "techs": ["Tableau"],
    "features": ["Interactive filters for dynamic data exploration", "Visual breakdown of flight performance and customer feedback", "User-friendly dashboard layout for quick insights"],
    "process": "This project was inspired by a tutorial I followed on YouTube, where I recreated the British Airways dashboard with slight modifications to the design and interactivity. The dataset provided information on flight performance, customer feedback, and operational metrics. I used Tableau’s visualization tools to build an interactive dashboard, enabling users to filter by parameters such as date, route, and rating to explore the data in detail.",
    "challenges": "Becoming familiar with Tableau’s advanced interactive features and customizing the dashboard beyond the tutorial example.",
    "learnings": "Gained hands-on experience with Tableau, dashboard design principles, and applying interactivity to enhance data storytelling."
  },
  "url-shortener": {
    "title": "URL Shortener",
    "image": "assets/images/portfolio/url-shortener.png",
    "description": "A lightweight tool to convert long, complex URLs into short and shareable links.",
    "link": "https://github.com/huszen/url-shortening-service",
    "techs": ["Python", "Flask", "SQLite"],
    "features": ["Generate short links instantly", "Redirect to original URLs", "Track click counts for each link"],
    "process": "I built this project to explore backend development and REST API creation. The core logic generates unique short codes for each submitted URL and stores the mapping in a database. When a user accesses the short link, the system redirects them to the original URL. I used Flask to create the server and API endpoints, and implemented a simple frontend for user interaction. SQLite was used for lightweight data storage, making the application easy to deploy.",
    "challenges": "Ensuring short code uniqueness, implementing efficient database lookups, and handling invalid or expired links gracefully.",
    "learnings": "Improved my skills in backend development, database design, and API deployment, while gaining a deeper understanding of URL routing and redirection."
  },
  "bike-analyst": {
    "title": "Urban Bike Usage Insights",
    "image": "assets/images/portfolio/urban_bike.png",
    "description": "A data analysis project exploring bike usage patterns in New York City using public ride data.",
    "link": "https://github.com/huszen/urban-bike-usage-insight",
    "techs": ["Python", "Pandas", "Matplotlib", "Seaborn", "Jupyter Notebook"],
    "features": ["Data cleaning and preprocessing", "Exploratory data analysis (EDA)", "Visualization of usage patterns by time, location, and user type"],
    "process": "This project focused on analyzing New York City bike usage data to uncover insights into rider behavior and trends. I began by cleaning and preprocessing the dataset to handle missing values and inconsistent entries. Then, I performed exploratory data analysis to identify patterns, such as peak usage hours, popular stations, and seasonal variations. The findings were visualized using Matplotlib and Seaborn for better clarity and storytelling.",
    "challenges": "Handling large datasets efficiently, addressing missing or inconsistent data, and presenting findings in a clear and visually appealing way.",
    "learnings": "Enhanced my skills in data cleaning, exploratory data analysis, and data visualization, while improving my ability to translate raw data into actionable insights."
  },
  "e-commerce-api": {
    "title": "E-Commerce API",
    "image": "assets/images/portfolio/e_commerce_api.webp",
    "description": "A simple RESTful API for an e-commerce platform built with Flask and SQLite. It provides essential features like user authentication, product browsing, cart management, and a simulated checkout flow. The API is backend-only with no frontend, and payment success is simulated locally instead of using real Stripe webhooks.",
    "link": "https://github.com/huszen/e-commerce-api",
    "techs": ["Python", "Flask", "Flask-JWT-Extended", "Flask-SQLAlchemy", "SQLite"],
    "features": [
      "User registration and login with JWT authentication",
      "Browse and search products",
      "Add and remove products from cart",
      "Simulated checkout and order placement",
      "Admin can add, update, and delete products",
      "Manage product prices and inventory"
    ],
    "process": "I designed the API using Flask Blueprints for modular routes and SQLAlchemy for database modeling. The backend includes authentication, product management, cart, and checkout flows. A seed script was created to populate the database with sample data. Development followed a step-by-step process: authentication first, then product endpoints, cart functionality, and finally checkout simulation.",
    "challenges": "One challenge was structuring the API to remain simple yet extensible. Another challenge was simulating Stripe payment flows without real webhook integration while keeping the checkout logic realistic. Managing authentication securely with JWT and ensuring cart-product relations worked smoothly was also a key challenge.",
    "learnings": "Through this project, I learned how to build a RESTful API from scratch with Flask, structure routes using Blueprints, implement JWT authentication, and manage relational data with SQLAlchemy. I also gained experience in simulating payment flows and thinking about how role-based access control could be implemented in future improvements."
  },
  "saas-customer-churn-analysist": {
    "title": "SaaS Customer Churn Analysist",
    "image": "assets/images/portfolio/aws-saas-churn.png",
    "description": "This project analyzes customer churn patterns in a fictitious SaaS sales dataset (AWS dataset). Using RFM analysis and machine learning, the project identifies factors contributing to churn and builds a predictive model.",
    "link": "https://github.com/huszen/aws-ficticious-saas-churn.git",
    "techs": ["Python", "Pandas", "Matplotlib", "Seaborn", "Scikit-learn", "Jupyter Notebook"],
    "features": ["Exploratory Data Analysis (EDA)", "RFM (Recency, Frequency, Monetary) analysis", "Churn labeling", "Correlation analysis of discounts vs. profits", "Churn visualization", "Machine learning model for churn prediction"],
    "process": "The project started with exploratory data analysis to understand sales trends and segment-level discount-profit correlations. Next, RFM features (Recency, Frequency, Monetary) were engineered to represent customer behavior. Churn was defined by labeling the top 25% most inactive customers as churned. Hypothesis testing was conducted to validate differences in RFM values between churned and active customers. Finally, a machine learning model was trained and evaluated to predict churn.",
    "challenges": "One challenge was defining churn in a meaningful way for a fictitious dataset. Since the data did not explicitly contain churn labels, a logical threshold based on recency had to be created. Another challenge was dealing with the small number of unique customers after grouping, which limited the dataset size and risked overfitting in the machine learning model.",
    "learnings": "From this project, I learned how to design churn labels using RFM analysis rather than arbitrary assumptions, and the importance of validating RFM features statistically before modeling. I also gained insights into how discounts can negatively affect profits in SaaS sales and built an end-to-end churn analysis pipeline covering EDA, RFM feature engineering, churn labeling, statistical validation, and machine learning."
  },
  "walmart-sales-forecast": {
    "title": "Walmart Sales Forecast",
    "image": "assets/images/portfolio/walmart-sales-forecast.png",
    "description": "A time series forecasting project using Walmart sales data. The project explores calendar features, lag-based predictors, and deep learning (PyTorch LSTM) to forecast future sales and compare different modeling approaches.",
    "link": "https://github.com/huszen/walmart-sales-forecast",
    "techs": ["Python", "Pandas", "Matplotlib", "Seaborn", "Scikit-learn", "PyTorch", "Jupyter Notebook"],
    "features": [
      "Exploratory Data Analysis (EDA) on Walmart sales dataset",
      "Feature engineering with calendar and lag-based features",
      "Store-level forecasting models",
      "Global forecasting model across multiple stores",
      "PyTorch LSTM sequence model for time series forecasting",
      "Evaluation using MAE, RMSE, and R² metrics",
      "Visualization of forecast vs. actual sales"
    ],
    "process": "The project began with exploratory data analysis to understand sales trends at the store level. Feature engineering was then applied by adding calendar variables (day, week, month) and lagged sales values. At first, the model was trained using only calendar features and focused on a particular store or department, without incorporating lagged values. The results, however, were not very promising. To improve performance, lagged features were introduced. Lag features are essentially past values of the target variable (e.g., sales from previous weeks), which provide temporal context and help the model capture sequential patterns. This model showed improvement compared to the initial one. Afterward, I expanded the training scope from store/department level to a global model, which aggregated data across stores. This approach further improved performance and gave insights into how sales forecasting behaves on a broader scale. Following this, I implemented a PyTorch model, starting with a simple linear regression built in PyTorch, which delivered better performance compared to the earlier scikit-learn models. Encouraged by this, I experimented with a more complex architecture by adding an LSTM layer to capture temporal dependencies. However, the LSTM model performed poorly. This was likely due to the nature of LSTM models, which require carefully structured sequential inputs and longer training to leverage their strengths. Since I only made minimal adjustments to the dataset for the LSTM input, the data was not fully optimized for sequence modeling, which led to underperformance.",
    "challenges": "The biggest challenge in this project was preparing the data to behave like true time series data. Since I wasn’t very familiar with time series at the start, I had to figure out how to transform the dataset so it could be used for forecasting. A key part of this was implementing a sliding window approach, which allowed me to create sequential samples that the model could learn from. Getting this step right was essential for building and training the forecasting models.",
    "learnings": "This project enhanced my understanding of time series forecasting, feature engineering, and deep learning with PyTorch. I learned the importance of starting with simple baseline models such as linear regression and global regressors before moving to more complex architectures. I also gained practical experience in reshaping data into a time series format using sliding windows, which allowed me to build forecasting models capable of predicting future sales."
  },
  "email-summarizer-agent": {
  "title": "Email Summarizer Agent",
  "image": "assets/images/portfolio/email_summarizer_agent.jpg",
  "description": "A Python automation tool that fetches recent emails, summarizes them using Google Gemini, and sends structured summaries back to your inbox.",
  "link": "https://github.com/huszen/email-summarizer-agent/",
  "techs": [
    "Python",
    "Gmail API",
    "Google Gemini API",
    "OAuth2",
    "IMAP/SMTP",
    "HTML Formatting"
  ],
  "features": [
    "Secure authentication using OAuth2",
    "Automatic retrieval of yesterday's emails",
    "Email text preprocessing and cleaning",
    "AI-powered summarization using Google Gemini",
    "HTML email formatting with headings and bullet points",
    "Automatic delivery of summary digest to user inbox"
  ],
  "process": "I began the project by outlining the core goal, to automatically read emails, summarize them using AI, and return a digest back to my inbox. I first planned the workflow and identified what needed to be built: authentication, email retrieval, text processing, AI summarization, formatting, and sending the output back to the user.\n\nI then configured secure authentication using OAuth2, which required creating a Google Cloud project, generating the credentials.json file, and handling token generation during the initial script execution. To make it more flexible, I also enabled IMAP/SMTP support so it could work with any email provider.\n\nOnce authentication was ready, I developed the Gmail service logic. It retrieves yesterday’s emails, parses relevant metadata, and extracts raw email bodies. I implemented preprocessing routines to clean noisy text, remove signatures, and standardize content before summarization.\n\nNext, I integrated the Google Gemini API for summarization. I experimented with multiple prompting approaches to produce concise yet meaningful summaries. After achieving reliable output, I built a formatter module that transforms summaries into readable HTML, including headings, bullet points, and spacing for easy scanning.\n\nI then created a sender module responsible for delivering the final formatted summary back to my inbox via automated email. I validated SMTP configurations, automated subject generation, and ensured that summary emails were delivered successfully.\n\nAfter assembling all pieces, I wrote the main script to orchestrate the entire workflow: authenticate, fetch emails, preprocess, summarize, format, and send. Throughout development, I tested each module individually and together, refined error handling, supported environment configuration via .env, and adjusted edge cases such as empty inboxes or malformed messages. The result is an autonomous summarization agent that runs end-to-end with a single command.",
  "challenges": [
    "Handling OAuth2 authentication and token refresh complexity for Gmail API.",
    "Cleaning inconsistent email body content and stripping signatures or HTML noise.",
    "Designing effective prompting strategies to ensure meaningful AI-generated summaries.",
    "Building robust formatting logic that worked for varied email content structures.",
    "Managing email sending reliability and SMTP configuration differences."
  ],
  "learnings": [
    "Gained practical experience integrating Gmail API and OAuth2 authentication.",
    "Improved skills in text preprocessing for unstructured email content.",
    "Learned prompt engineering techniques to achieve clear summarization outputs.",
    "Deepened understanding of IMAP, SMTP, and automated message delivery workflows.",
    "Strengthened modular Python development practices and environment configuration."
  ]
}
}
