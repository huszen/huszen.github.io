{
  "gemini-chatbot": {
    "title": "Gemini PDF RAG",
    "image": "assets/images/portfolio/gemini_pdf_rag.png",
    "description": "Chat with any PDF using Gemini + LangChain. Upload your PDF, ask questions, and get contextual answers.",
    "link": "https://github.com/huszen/gemini_pdf_rag",
    "techs": ["LangChain", "Google AI Studio", "Streamlit", "Python"],
    "features": ["PDF upload and parsing", "RAG-based Q&A", "Streaming response", "Multi-file support"],
    "process": "I built this project because I wanted to explore whether I could use Google AI Studio for my own personal projects. Specifically, I was curious to know if it's possible to integrate the API from Google AI Studio into a custom application. Around the same time, I was also learning about LangChain, and the idea came to mind: maybe I could build something useful, lightweight, and free for myself. The main goal was to interact with PDF files using a large language model (LLM) without having to download or host any of the heavy models myself. Thanks to Google AI Studio, I could use their hosted model through an API key, which made everything much simpler.That’s how my journey to build this project started. The first major step was creating the core of a Retrieval-Augmented Generation (RAG) system using LangChain. I began by figuring out how to read and extract content from PDF files. Fortunately, LangChain supports the PyPDF library, which made it easy to load and process PDFs. After reading the file, the next step was embedding it—this means converting the content into a numerical format that the LLM can understand and retrieve from. Once embedded, I split the content into smaller chunks. These chunks act as the pieces of information that LangChain can retrieve later, and they serve as context for the model when answering user queries.After chunking the PDF, I created a custom prompt template. This prompt helps guide the LLM to respond more accurately by including not only the user's question but also the relevant context from the PDF. The result is a more focused and document-aware response. Although I attempted to add conversational memory to the system (so it could remember past interactions), I haven’t fully implemented that part yet.Once the core RAG logic was complete and working well, I moved on to building a simple web interface using Streamlit. This made the project easy to use—users can upload their PDF files, type their questions, and receive responses generated by Google’s language model, all within a lightweight app.One of the best things about this project is how lightweight it is. There's no need to download or host any large models—just an API key from Google AI Studio is enough. On the downside, I noticed that embedding very large PDF files might take quite a long time and may not yield the best results (though I haven’t fully tested that yet). Overall, I’m really happy with how it turned out: I built a working RAG system, learned more about LangChain, and proved that it’s possible to create useful AI tools with minimal resources.",
    "challenges": "One of the main challenges I faced was designing a user-friendly chat interface that provides a smooth and intuitive experience. I wanted the interaction to feel natural, especially since users would be engaging with an AI to ask questions about their documents. Another significant challenge was implementing a memory system. Ideally, the LLM should be able to remember previous messages so it can understand follow-up questions or references to earlier parts of the conversation. Building this kind of contextual memory is still a work in progress, as it requires maintaining and managing conversation history in a way that’s both efficient and compatible with the model’s input limitations.",
    "learnings": "Improved my understanding of LLMs, API token handling, and file parsing."
  },
  "keyphrase-generation": {
    "title": "Keyphrase Generation with T5 and BART",
    "image": "assets/images/portfolio/keyphrase_gen_assets.gif",
    "description": "Fine-tuned T5 and BART to automatically generate keyphrases for scientific documents.",
    "link": "https://github.com/huszen/keyphrase-generation-with-t5-and-bart-model",
    "techs": ["T5", "BART", "HuggingFace Transformers", "PyTorch"],
    "features": ["Supports both extraction & generation of keyphrases", "Trained on custom datasets", "F1 evaluation"],
    "process": "This project was part of my exploration into fine-tuning language models for keyphrase generation. My goal was to make the model generate keyphrases from scientific texts based on how I wanted it to behave. I used T5 and BART—both encoder-decoder transformers capable of understanding context and generating text. I fine-tuned these models using Hugging Face’s codebase, mostly adjusting and modifying existing scripts to fit my use case, so there wasn't much custom coding involved. For training, I used the KP20K dataset, which is commonly used for keyphrase generation tasks. Although the final results weren’t ideal—the F1 scores were lower than expected—it was a valuable learning experience that taught me the fundamentals of fine-tuning transformer models.",
    "challenges": "Finding optimal hyperparameters was difficult, and the long training time made it hard to experiment with different configurations.",
    "learnings": "Deepened my understanding of pre-trained language models and the fine-tuning process for custom NLP tasks."
  },
  "visit-campus": {
    "title": "Visit Campus",
    "image": "assets/images/portfolio/visitCampus_cover.png",
    "description": "A simple app to help you learn more about your dream college",
    "link": "https://github.com/sugama1001/VisitCampus",
    "techs": ["Python", "Tensorflow", "Flask"],
    "features": ["ChatBot", "Personality Test", "Information System"],
    "process": "This is our Bangkit Capstone Project. I was responsible for creating two machine learning models along with two of my peers. The first was a personality test model, designed to help users better understand themselves and discover which degree program might suit them. The second was a chatbot model, created to assist users in learning more about our application. After developing these models, our machine learning cohort also worked on building API endpoints to deliver the model outputs to end users. We used Flask to create the REST API for this purpose.",
    "challenges": "Building a robust personality prediction model; creating a chatbot model capable of answering specific questions about our application; developing API endpoints for both models, which was our first experience in API development.",
    "learnings": "Deepened my understanding of end-to-end production, from backend to frontend."
  },
  "british-airways-dashboard": {
    "title": "British Airways Dashboard with Tableau",
    "image": "assets/images/portfolio/british_airways_dashboard.png",
    "description": "An interactive Tableau dashboard visualizing British Airways data, with filtering and drill-down features.",
    "link": "https://public.tableau.com/app/profile/hasbi.hussein.deri/viz/my_airplane_analysis/Dashboard",
    "techs": ["Tableau"],
    "features": ["Interactive filters for dynamic data exploration", "Visual breakdown of flight performance and customer feedback", "User-friendly dashboard layout for quick insights"],
    "process": "This project was inspired by a tutorial I followed on YouTube, where I recreated the British Airways dashboard with slight modifications to the design and interactivity. The dataset provided information on flight performance, customer feedback, and operational metrics. I used Tableau’s visualization tools to build an interactive dashboard, enabling users to filter by parameters such as date, route, and rating to explore the data in detail.",
    "challenges": "Becoming familiar with Tableau’s advanced interactive features and customizing the dashboard beyond the tutorial example.",
    "learnings": "Gained hands-on experience with Tableau, dashboard design principles, and applying interactivity to enhance data storytelling."
  },
  "url-shortener": {
    "title": "URL Shortener",
    "image": "assets/images/portfolio/url-shortener.png",
    "description": "A lightweight tool to convert long, complex URLs into short and shareable links.",
    "link": "https://github.com/huszen/url-shortening-service",
    "techs": ["Python", "Flask", "SQLite"],
    "features": ["Generate short links instantly", "Redirect to original URLs", "Track click counts for each link"],
    "process": "I built this project to explore backend development and REST API creation. The core logic generates unique short codes for each submitted URL and stores the mapping in a database. When a user accesses the short link, the system redirects them to the original URL. I used Flask to create the server and API endpoints, and implemented a simple frontend for user interaction. SQLite was used for lightweight data storage, making the application easy to deploy.",
    "challenges": "Ensuring short code uniqueness, implementing efficient database lookups, and handling invalid or expired links gracefully.",
    "learnings": "Improved my skills in backend development, database design, and API deployment, while gaining a deeper understanding of URL routing and redirection."
  },
  "bike-analyst": {
    "title": "Urban Bike Usage Insights",
    "image": "assets/images/portfolio/urban_bike.png",
    "description": "A data analysis project exploring bike usage patterns in New York City using public ride data.",
    "link": "https://github.com/huszen/urban-bike-usage-insight",
    "techs": ["Python", "Pandas", "Matplotlib", "Seaborn", "Jupyter Notebook"],
    "features": ["Data cleaning and preprocessing", "Exploratory data analysis (EDA)", "Visualization of usage patterns by time, location, and user type"],
    "process": "This project focused on analyzing New York City bike usage data to uncover insights into rider behavior and trends. I began by cleaning and preprocessing the dataset to handle missing values and inconsistent entries. Then, I performed exploratory data analysis to identify patterns, such as peak usage hours, popular stations, and seasonal variations. The findings were visualized using Matplotlib and Seaborn for better clarity and storytelling.",
    "challenges": "Handling large datasets efficiently, addressing missing or inconsistent data, and presenting findings in a clear and visually appealing way.",
    "learnings": "Enhanced my skills in data cleaning, exploratory data analysis, and data visualization, while improving my ability to translate raw data into actionable insights."
  },
  "e-commerce-api": {
    "title": "E-Commerce API",
    "image": "assets/images/portfolio/e_commerce_api.webp",
    "description": "A simple RESTful API for an e-commerce platform built with Flask and SQLite. It provides essential features like user authentication, product browsing, cart management, and a simulated checkout flow. The API is backend-only with no frontend, and payment success is simulated locally instead of using real Stripe webhooks.",
    "link": "https://github.com/huszen/e-commerce-api",
    "techs": ["Python", "Flask", "Flask-JWT-Extended", "Flask-SQLAlchemy", "SQLite"],
    "features": [
      "User registration and login with JWT authentication",
      "Browse and search products",
      "Add and remove products from cart",
      "Simulated checkout and order placement",
      "Admin can add, update, and delete products",
      "Manage product prices and inventory"
    ],
    "process": "I designed the API using Flask Blueprints for modular routes and SQLAlchemy for database modeling. The backend includes authentication, product management, cart, and checkout flows. A seed script was created to populate the database with sample data. Development followed a step-by-step process: authentication first, then product endpoints, cart functionality, and finally checkout simulation.",
    "challenges": "One challenge was structuring the API to remain simple yet extensible. Another challenge was simulating Stripe payment flows without real webhook integration while keeping the checkout logic realistic. Managing authentication securely with JWT and ensuring cart-product relations worked smoothly was also a key challenge.",
    "learnings": "Through this project, I learned how to build a RESTful API from scratch with Flask, structure routes using Blueprints, implement JWT authentication, and manage relational data with SQLAlchemy. I also gained experience in simulating payment flows and thinking about how role-based access control could be implemented in future improvements."
  }
}
